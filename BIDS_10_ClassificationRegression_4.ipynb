{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim to complete as much of this tutorial on your own *before* coming to the practical session.\n",
    "\n",
    "Use the practical session to get help for any aspect you do not understand or were unable to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning objectives\n",
    "1. Apply single- and multilayer FNNs in a binary classification setting using the popular python library [sklearn](https://scikit-learn.org/stable/)\n",
    "2. Visualise the important variables of the single-layer FNN \n",
    "3. Explore different metrics to evaluate the model performance in regression settings\n",
    "4. Explore the effect of changing the network architecture, model parameters and scaling on the model performance and training times\n",
    "\n",
    "Optional learning objective (view the video online about AEs before you start)\n",
    "\n",
    "5. Apply AE (autoencoder) for data reduction and using the output for a classification model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as mx\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use some plasma metabolomics data to predict type-II diabetes occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data-main/diabetes_metabolomics_plasma.csv' ) #  can change this to the previous xlsx format  below \n",
    "#  pd.read_excel('../Data/diabetes_metabolomics_plasma.xlsx'), add or remove the index_col argument if necissary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ETH</th>\n",
       "      <th>T2D</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>1,11-Undecanedicarboxylic acid</th>\n",
       "      <th>1,2-dipalmitoylglycerol</th>\n",
       "      <th>1,2-propanediol</th>\n",
       "      <th>1,3,7-trimethylurate</th>\n",
       "      <th>...</th>\n",
       "      <th>tyrosine</th>\n",
       "      <th>undecanoate (11:0)</th>\n",
       "      <th>urate</th>\n",
       "      <th>urea</th>\n",
       "      <th>uridine</th>\n",
       "      <th>valine</th>\n",
       "      <th>xanthine</th>\n",
       "      <th>xylitol</th>\n",
       "      <th>xylonate</th>\n",
       "      <th>xylose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>QMDiab-ID222</td>\n",
       "      <td>4.015998</td>\n",
       "      <td>4.822787</td>\n",
       "      <td>5.098491</td>\n",
       "      <td>4.430118</td>\n",
       "      <td>...</td>\n",
       "      <td>6.844272</td>\n",
       "      <td>4.885885</td>\n",
       "      <td>5.824023</td>\n",
       "      <td>7.577408</td>\n",
       "      <td>5.290736</td>\n",
       "      <td>7.239540</td>\n",
       "      <td>4.254777</td>\n",
       "      <td>4.052859</td>\n",
       "      <td>4.387091</td>\n",
       "      <td>4.676128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>QMDiab-ID113</td>\n",
       "      <td>4.165116</td>\n",
       "      <td>4.991613</td>\n",
       "      <td>5.064084</td>\n",
       "      <td>4.585177</td>\n",
       "      <td>...</td>\n",
       "      <td>6.755005</td>\n",
       "      <td>4.777224</td>\n",
       "      <td>5.990350</td>\n",
       "      <td>7.465627</td>\n",
       "      <td>5.220019</td>\n",
       "      <td>7.348071</td>\n",
       "      <td>4.183861</td>\n",
       "      <td>4.341484</td>\n",
       "      <td>4.460427</td>\n",
       "      <td>4.585401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>29.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>QMDiab-ID29</td>\n",
       "      <td>3.928917</td>\n",
       "      <td>4.965361</td>\n",
       "      <td>5.185007</td>\n",
       "      <td>4.006547</td>\n",
       "      <td>...</td>\n",
       "      <td>6.971284</td>\n",
       "      <td>4.737634</td>\n",
       "      <td>5.892525</td>\n",
       "      <td>7.433326</td>\n",
       "      <td>5.188568</td>\n",
       "      <td>7.356957</td>\n",
       "      <td>4.350213</td>\n",
       "      <td>4.101610</td>\n",
       "      <td>4.726884</td>\n",
       "      <td>4.785554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>QMDiab-ID243</td>\n",
       "      <td>4.272476</td>\n",
       "      <td>4.819821</td>\n",
       "      <td>5.015473</td>\n",
       "      <td>4.564346</td>\n",
       "      <td>...</td>\n",
       "      <td>6.943175</td>\n",
       "      <td>4.736576</td>\n",
       "      <td>5.832160</td>\n",
       "      <td>7.422004</td>\n",
       "      <td>5.037060</td>\n",
       "      <td>7.260422</td>\n",
       "      <td>4.037787</td>\n",
       "      <td>4.111296</td>\n",
       "      <td>4.488342</td>\n",
       "      <td>4.696762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>QMDiab-ID270</td>\n",
       "      <td>3.914247</td>\n",
       "      <td>4.900546</td>\n",
       "      <td>5.264884</td>\n",
       "      <td>3.719528</td>\n",
       "      <td>...</td>\n",
       "      <td>7.068370</td>\n",
       "      <td>4.754971</td>\n",
       "      <td>6.013982</td>\n",
       "      <td>7.786201</td>\n",
       "      <td>5.388167</td>\n",
       "      <td>7.377851</td>\n",
       "      <td>4.211793</td>\n",
       "      <td>4.099279</td>\n",
       "      <td>4.545646</td>\n",
       "      <td>4.630343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE  GENDER   BMI  ETH  T2D     sample_id  1,11-Undecanedicarboxylic acid  \\\n",
       "0   35       0  25.0    2    0  QMDiab-ID222                        4.015998   \n",
       "1   47       1  28.4    3    0  QMDiab-ID113                        4.165116   \n",
       "2   55       1  29.7    1    0   QMDiab-ID29                        3.928917   \n",
       "3   56       1  23.1    2    0  QMDiab-ID243                        4.272476   \n",
       "4   36       1  30.1    1    0  QMDiab-ID270                        3.914247   \n",
       "\n",
       "   1,2-dipalmitoylglycerol  1,2-propanediol  1,3,7-trimethylurate  ...  \\\n",
       "0                 4.822787         5.098491              4.430118  ...   \n",
       "1                 4.991613         5.064084              4.585177  ...   \n",
       "2                 4.965361         5.185007              4.006547  ...   \n",
       "3                 4.819821         5.015473              4.564346  ...   \n",
       "4                 4.900546         5.264884              3.719528  ...   \n",
       "\n",
       "   tyrosine  undecanoate (11:0)     urate      urea   uridine    valine  \\\n",
       "0  6.844272            4.885885  5.824023  7.577408  5.290736  7.239540   \n",
       "1  6.755005            4.777224  5.990350  7.465627  5.220019  7.348071   \n",
       "2  6.971284            4.737634  5.892525  7.433326  5.188568  7.356957   \n",
       "3  6.943175            4.736576  5.832160  7.422004  5.037060  7.260422   \n",
       "4  7.068370            4.754971  6.013982  7.786201  5.388167  7.377851   \n",
       "\n",
       "   xanthine   xylitol  xylonate    xylose  \n",
       "0  4.254777  4.052859  4.387091  4.676128  \n",
       "1  4.183861  4.341484  4.460427  4.585401  \n",
       "2  4.350213  4.101610  4.726884  4.785554  \n",
       "3  4.037787  4.111296  4.488342  4.696762  \n",
       "4  4.211793  4.099279  4.545646  4.630343  \n",
       "\n",
       "[5 rows x 328 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['T2D'].value_counts())\n",
    "df['T2D'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python column indices start from 0, we want to subset only the metabolomics colulmns for feature scaling. \n",
    "As you see the data is fairly balanced, something to think about is how do our metrics change when there is a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix and target vector\n",
    "X = df.iloc[:,6:]\n",
    "y = df['T2D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our alogrithms we need to set aside some of the data we have. This is practice for machine learning models. We will use 80% of our data to train our model, and the remaining 20% will be used to test the performance of our model. \n",
    "\n",
    "Scikit-Learn has a function ```train_test_split``` to easily do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your CID here, or date of birth, or another number of your choosing to use as random state\n",
    "CID = 0\n",
    "\n",
    "# remember to check the documentation of each algorithm if setting the random_state is needed\n",
    "# for this tutorial and all upcoming tutorials..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into 80% train 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that all of the data it is comparing is on the same scale. In our proteomics data, most of the data is continuous. We will scale the data using the [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) shown in the previous tutorials. \n",
    "\n",
    "When scaling your data you want to fit the model to your training data, and only transform your testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scaler model\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and Transform X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating model performance we want to start with a baseline accuracy. This is the accuracy score if we were to simply guess the majority outcome everytime. It gives us a starting point to compare our models to. The baseline metric is the best we can do without models. Hopefully, our models can improve over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the baseline accuracy\n",
    "\n",
    "# Find the majority count\n",
    "y_train.value_counts()# Calculate the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Counts for y_test\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we were to guess the majority (0) for each test, we would get 42 correct\n",
    "# baseline is correct guesses divided by total guesses \n",
    "baseline = 38 / (38 + 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "38 / (38 + 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2 Diabetes classification using  MLPClassifier from Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the MLPClassifier (this one is just a single lauer of 100 neurons )\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100), max_iter=300,activation = 'relu',solver='adam',random_state=42) ### single layer of 100 neruons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the training data to the network\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Using the trained network to predict\n",
    "\n",
    "#Predicting y for X_val\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how the coeficents at each layer change-- have a think about why the shape changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coefs_[1].shape  ### Change here the number from try 1 and 0 and look at the shape difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(classifier.coefs_[0][0:20], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(20), np.array(X.columns)[0:20])\n",
    "plt.xlabel(\"Columns in weight matrix\")\n",
    "plt.ylabel(\"Input feature\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Figure\n",
    "\n",
    "- Shows weights learned connecting the input to the first hidden layer\n",
    "- Rows in the plot correspond to the 20 input features\n",
    "- Columns in plot correspond to the 100 hidden units\n",
    "\n",
    "\n",
    "- Feature that have very small weights for all hidden units are 'less important' to model\n",
    "- Could also visualize weight connecting the hidden layers to the output layer, but that is even harder to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy is one perfomrance metric here we will define several alternative metrics \n",
    "\n",
    "def modelPerformance(confMat):\n",
    "    TN = confMat[0, 0]\n",
    "    TP = confMat[1, 1]\n",
    "    FP = confMat[0, 1]\n",
    "    FN = confMat[1, 0]\n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    spec = TN / (TN + FP)\n",
    "    fpr = FP / (TN + FP)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    return (acc, prec, rec, spec, fpr, f1)\n",
    "\n",
    "def printPerformance(confMat):\n",
    "    acc, prec, rec, spec, fpr, f1 = modelPerformance(confMat)\n",
    "    print(\"Accuracy = \" \"%.4f\" % acc)\n",
    "    print(\"Precision = \" \"%.4f\" % prec)\n",
    "    print(\"Recall = \" \"%.4f\" % rec)\n",
    "    print(\"Specificity = \" \"%.4f\" % spec)\n",
    "    print(\"False positive rate = \" \"%.4f\" % fpr)\n",
    "    print(\"F1-score = \" \"%.4f\" % f1)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"Confusion matrix (%):\")\n",
    "    print(confMat/np.sum(confMat)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are achieving an accuracy of 0.82, lets see if we can improve upon this with more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(100,200), max_iter=300,activation = 'relu',solver='adam',random_state=42) ###  layer of 100 and 200 neruons \n",
    "#Fitting the training data to the network\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Using the trained network to predict\n",
    "\n",
    "#Predicting y for X_val\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Confusion matrix and metrics\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are achieving an accuracy of 0.83, lets see if we can improve upon this with 10-fold CV and some hyperparmaters tuning. We can use the GridSearchCV fucntion to help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[42], 'activation':['relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting y for X_val\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we can achieve a resonable test Accuracy. Now try  in the cel below some of your own hyperparameters in the gridsearch setting and see if you can achieve a higher performance. \n",
    "This documentation will help https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try it yourself here with parameters of your choice \n",
    "parameters = {'solver': ['adam'], 'max_iter': [123,456,789], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(1,2),(3,4)] , 'random_state':[42], 'activation':['relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "### this may take 2-5 minutes to run depending on your hardware and no. hyperperamters\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "#Predicting y for X_val\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional materials\n",
    "Please view the video online about AEs before you start this part of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have understood how to implement FNNS, let's aim to implement another common neural network: an autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders (AEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make an simple autoencoder from the MLPRegressor function in sklearn\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of latent variable and hidden layers\n",
    "\n",
    "# Encoder structure\n",
    "n_encoder1 = 300\n",
    "n_encoder2 = 500\n",
    "\n",
    "n_latent = 50\n",
    "\n",
    "# Decoder structure\n",
    "n_decoder2 = 300\n",
    "n_decoder1 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make an simple autoencoder from the MLPRegressor function in sklearn\n",
    "reg = MLPRegressor(hidden_layer_sizes = (n_encoder1, n_encoder2, n_latent, n_decoder2, n_decoder1), \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam', \n",
    "                   learning_rate_init = 0.001, \n",
    "                   max_iter = 100, # This is where we define the no.itterations/epochs in sklearn\n",
    "                   tol = 0.000001, \n",
    "                   verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with trainin data  as input an dX as output \n",
    "# This may take 5 minutes depending on your hardware, if training takes too long decrease max_iter in the above cell\n",
    "# It may not converge but for this tutorial that will be fine \n",
    "reg.fit(X_train_scaled, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder helper function, no need to change this \n",
    "\n",
    "def encoder(data):\n",
    "    data = np.asmatrix(data)\n",
    "    \n",
    "    encoder1 = data*reg.coefs_[0] + reg.intercepts_[0]\n",
    "    encoder1 = (np.exp(encoder1) - np.exp(-encoder1))/(np.exp(encoder1) + np.exp(-encoder1))\n",
    "    \n",
    "    encoder2 = encoder1*reg.coefs_[1] + reg.intercepts_[1]\n",
    "    encoder2 = (np.exp(encoder2) - np.exp(-encoder2))/(np.exp(encoder2) + np.exp(-encoder2))\n",
    "    \n",
    "    latent = encoder2*reg.coefs_[2] + reg.intercepts_[2]\n",
    "    latent = (np.exp(latent) - np.exp(-latent))/(np.exp(latent) + np.exp(-latent))\n",
    "    \n",
    "    return np.asarray(latent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Latent Space of the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent embeddings with trained encoder part of autoencoder\n",
    "test_latent = encoder(X_test_scaled)\n",
    "train_latent = encoder(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_latent[:,0], train_latent[:,1], c=y_train, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in previous tutorials we can also examine this latent space with PCA compnents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, SparsePCA, KernelPCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_latent = pca.fit_transform(train_latent)\n",
    "# run PCA with 4 components\n",
    "# plot a scatterplot using seaborn\n",
    "# the x axis will contain the first column of the pca scores x=pca_covid[:, 0]\n",
    "plt.scatter(pca_latent[:, 0], pca_latent[:, 1], c=y_train, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use SparsePCA, KernelPCA, UMAP etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now train an FFNN on this autoencoder generated latent space to predict T2 Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[42], 'activation':['relu']}\n",
    "clf_lat = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf_lat.fit(train_latent, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting y for X_val\n",
    "y_pred = clf_lat.predict(test_latent)\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is not performing as well as some of our previous models, however we have not tuned the hyperparamaters of this model. This can take a few hours to train an autoencoder to get a good performance. In an ideal setting we would itterate over various paramters however for today you can try change the dimensions of the latent space and see if you can get an improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use PCA etc. for dimensionality reduction and build models on the PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "train_components = pca.fit_transform(X_train_scaled)\n",
    "test_components = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[42], 'activation':['relu']}\n",
    "clf_pca= GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf_pca.fit(train_components, y_train)\n",
    "\n",
    "\n",
    "#Predicting y for test data \n",
    "y_pred = clf_pca.predict(test_components)\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the autoencoder did not perform the best (depsite being a neuralnet), perhaps with further hyperparamter tuning and training it would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the baseline test accuracy was 54%, our nerual networks have shown an improvement with and accuracy of 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next?\n",
    "We have walked through how to implement MLPClassifiers, MLPRegression (autoencoder) \n",
    "- Its important to note the MLPClassifier and MLPRegression only capture a small amount of what is possible with deep learning \n",
    "For further understanding and practice:\n",
    "- Try using a different scaling for instance: ```robust_scale``` / ```RobustScaler```, ```power_transform``` / ```PowerTransformer```\n",
    "- Change certain parameters such as latent space dimensions (number of components) in PCA\n",
    "- Use a different dataset for a regression problem\n",
    "- Try run a GridSearchCV on the Autoencoder model (note this will increase training time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

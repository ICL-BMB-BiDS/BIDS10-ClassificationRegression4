{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim to complete as much of this tutorial on your own *before* coming to the practical session.\n",
    "\n",
    "Use the practical session to get help for any aspect you do not understand or were unable to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning objectives\n",
    "1. Apply single- and multilayer FNNs in a binary classification setting using the popular python library [sklearn](https://scikit-learn.org/stable/)\n",
    "2. Visualise the important variables of the single-layer FNN \n",
    "3. Explore different metrics to evaluate the model performance in regression settings\n",
    "4. Explore the effect of changing the network architecture, model parameters and scaling on the model performance and training times\n",
    "\n",
    "Optional learning objective (view the video online about AEs before you start)\n",
    "\n",
    "5. Apply AE (autoencoder) for data reduction and using the output for a classification model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as mx\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use some plasma metabolomics data to predict type-II diabetes occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data-main/diabetes_metabolomics_plasma.csv' ) #  can change this to the previous xlsx format  below \n",
    "#  pd.read_excel('../Data/diabetes_metabolomics_plasma.xlsx'), add or remove the index_col argument if necissary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['T2D'].value_counts())\n",
    "df['T2D'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python column indices start from 0, we want to subset only the metabolomics colulmns for feature scaling. \n",
    "As you see the data is fairly balanced, something to think about is how do our metrics change when there is a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix and target vector\n",
    "X = df.iloc[:,6:]\n",
    "y = df['T2D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our alogrithms we need to set aside some of the data we have. This is practice for machine learning models. We will use 80% of our data to train our model, and the remaining 20% will be used to test the performance of our model. \n",
    "\n",
    "Scikit-Learn has a function ```train_test_split``` to easily do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your CID here, or date of birth, or another number of your choosing to use as random state\n",
    "CID = 0\n",
    "\n",
    "# remember to check the documentation of each algorithm if setting the random_state is needed\n",
    "# for this tutorial and all upcoming tutorials..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into 80% train 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=CID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial that all of the data it is comparing is on the same scale. In our proteomics data, most of the data is continuous. We will scale the data using the [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) shown in the previous tutorials. \n",
    "\n",
    "When scaling your data you want to fit the model to your training data, and only transform your testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scaler model\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating model performance we want to start with a baseline accuracy. This is the accuracy score if we were to simply guess the majority outcome everytime. It gives us a starting point to compare our models to. The baseline metric is the best we can do without models. Hopefully, our models can improve over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the baseline accuracy\n",
    "\n",
    "# Find the majority count\n",
    "y_train.value_counts()# Calculate the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Counts for y_test\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we were to guess the majority (0) for each test, we would get 42 correct\n",
    "# baseline is correct guesses divided by total guesses \n",
    "baseline = 38 / (38 + 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "38 / (38 + 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-2 diabetes classification using  MLPClassifier from Sklearn \n",
    "\n",
    "We will use the [MLPClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) function to create single hidden-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the MLPClassifier (this one is just a single layer of 100 neurons)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100), max_iter=300, activation='relu', solver='adam', random_state=CID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the training data to the network\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the trained network to predict y for the test set\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how the coeficents at each layer change, have a think about why the shape changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coefs_[1].shape  ### Change here the number from try 1 and 0 and look at the shape difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(classifier.coefs_[0][0:20], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(20), np.array(X.columns)[0:20])\n",
    "plt.xlabel(\"Columns in weight matrix\")\n",
    "plt.ylabel(\"Input feature\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Figure\n",
    "\n",
    "- Shows weights learned connecting the input to the first hidden layer\n",
    "- Rows in the plot correspond to the 20 input features\n",
    "- Columns in plot correspond to the 100 hidden units\n",
    "\n",
    "\n",
    "- Feature that have very small weights for all hidden units are 'less important' to model\n",
    "- Could also visualize weight connecting the hidden layers to the output layer, but that is even harder to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy is one performance metric here we will define several alternative metrics \n",
    "\n",
    "def modelPerformance(confMat):\n",
    "    TN = confMat[0, 0]\n",
    "    TP = confMat[1, 1]\n",
    "    FP = confMat[0, 1]\n",
    "    FN = confMat[1, 0]\n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    spec = TN / (TN + FP)\n",
    "    fpr = FP / (TN + FP)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    return (acc, prec, rec, spec, fpr, f1)\n",
    "\n",
    "def printPerformance(confMat):\n",
    "    acc, prec, rec, spec, fpr, f1 = modelPerformance(confMat)\n",
    "    print(\"Accuracy = \" \"%.4f\" % acc)\n",
    "    print(\"Precision = \" \"%.4f\" % prec)\n",
    "    print(\"Recall = \" \"%.4f\" % rec)\n",
    "    print(\"Specificity = \" \"%.4f\" % spec)\n",
    "    print(\"False positive rate = \" \"%.4f\" % fpr)\n",
    "    print(\"F1-score = \" \"%.4f\" % f1)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"Confusion matrix (%):\")\n",
    "    print(confMat/np.sum(confMat)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are achieving an accuracy of 0.82, lets see if we can improve upon this with more layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the MLPClassifier (this one has layers of 100 and 200 neurons)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,200), max_iter=300, activation='relu', solver='adam', random_state=CID)\n",
    "\n",
    "# Fitting the training data to the network\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the trained network to predict y for the test set\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and metrics\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are achieving an accuracy of 0.83, lets see if we can improve upon this with 10-fold CV and some hyperparameter tuning. We can use the [GridSearchCV()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function to help with this. Remember you can change `GridSearchCV` below for another method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[CID], 'activation':['relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting y for X_val\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we can achieve a reasonable test accuracy. Now try in the cell below some of your own hyperparameters in the gridsearch setting and see if you can achieve a higher performance?\n",
    "This [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) will help you choose different options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try it yourself here with parameters of your choice \n",
    "parameters = {'solver': ['adam'], 'max_iter': [123,456,789], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(1,2),(3,4)] , 'random_state':[42], 'activation':['relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "### this may take 2-5 minutes to run depending on your hardware and no. hyperperamters\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the trained network to predict y for the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional materials\n",
    "Please view the [Youtube video](https://youtu.be/k8TID4tvew8) online about AEs before you start this part of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have understood how to implement FNNs, let's aim to implement another common neural network: an autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders (AEs)\n",
    "\n",
    "We will use the [MLPRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make an simple autoencoder from the MLPRegressor function in sklearn\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of latent variable and hidden layers\n",
    "\n",
    "# Encoder structure\n",
    "n_encoder1 = 300\n",
    "n_encoder2 = 500\n",
    "\n",
    "n_latent = 50\n",
    "\n",
    "# Decoder structure\n",
    "n_decoder2 = 300\n",
    "n_decoder1 = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the documentation whether or not you need to set a `random_state` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make an simple autoencoder from the MLPRegressor function in sklearn\n",
    "reg = MLPRegressor(hidden_layer_sizes = (n_encoder1, n_encoder2, n_latent, n_decoder2, n_decoder1), \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam', \n",
    "                   learning_rate_init = 0.001, \n",
    "                   max_iter = 100, # This is where we define the number of iterations (or epochs) in sklearn\n",
    "                   tol = 0.000001, \n",
    "                   verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with trainin data  as input an dX as output \n",
    "# This may take 5 minutes depending on your hardware, if training takes too long decrease max_iter in the above cell\n",
    "# It may not converge but for this tutorial that will be fine \n",
    "reg.fit(X_train_scaled, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder helper function, no need to change this \n",
    "\n",
    "def encoder(data):\n",
    "    data = np.asmatrix(data)\n",
    "    \n",
    "    encoder1 = data*reg.coefs_[0] + reg.intercepts_[0]\n",
    "    encoder1 = (np.exp(encoder1) - np.exp(-encoder1))/(np.exp(encoder1) + np.exp(-encoder1))\n",
    "    \n",
    "    encoder2 = encoder1*reg.coefs_[1] + reg.intercepts_[1]\n",
    "    encoder2 = (np.exp(encoder2) - np.exp(-encoder2))/(np.exp(encoder2) + np.exp(-encoder2))\n",
    "    \n",
    "    latent = encoder2*reg.coefs_[2] + reg.intercepts_[2]\n",
    "    latent = (np.exp(latent) - np.exp(-latent))/(np.exp(latent) + np.exp(-latent))\n",
    "    \n",
    "    return np.asarray(latent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Latent Space of the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent embeddings with trained encoder part of autoencoder\n",
    "test_latent = encoder(X_test_scaled)\n",
    "train_latent = encoder(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_latent[:,0], train_latent[:,1], c=y_train, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in previous tutorials we can also examine this latent space with PCA compnents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, SparsePCA, KernelPCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_latent = pca.fit_transform(train_latent)\n",
    "# run PCA with 4 components\n",
    "# plot a scatterplot using seaborn\n",
    "# the x axis will contain the first column of the pca scores x=pca_covid[:, 0]\n",
    "plt.scatter(pca_latent[:, 0], pca_latent[:, 1], c=y_train, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use _any_ of the methods from BIDS 2-4 (e.g. [SparsePCA()](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html), [Kernel PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA), [umap.UMAP()](https://umap-learn.readthedocs.io/en/latest/index.html), etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now train an FFNN on this autoencoder generated latent space to predict T2 Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[CID], 'activation':['relu']}\n",
    "clf_lat = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf_lat.fit(train_latent, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting y for X_val\n",
    "y_pred = clf_lat.predict(test_latent)\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is not performing as well as some of our previous models, however we have not tuned the hyperparamaters of this model. This can take a few hours to train an autoencoder to get a good performance. In an ideal setting we would itterate over various paramters however for today you can try change the dimensions of the latent space and see if you can get an improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use PCA (or other methods) for dimensionality reduction and build models on the PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50) # if you've changed the latent space dimension for AEs, change it here too\n",
    "train_components = pca.fit_transform(X_train_scaled)\n",
    "test_components = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember you can change `GridSearchCV` below for another method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['adam'], 'max_iter': [300,500,700], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':[(50,100),(100,200),(50, 100,200), (100,200,300)] , 'random_state':[CID], 'activation':['relu']}\n",
    "clf_pca= GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "### this may take 2-5 minutes to run depending on your hardware\n",
    "clf_pca.fit(train_components, y_train)\n",
    "\n",
    "#Predicting y for test data \n",
    "y_pred = clf_pca.predict(test_components)\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "printPerformance(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the autoencoder did not perform the best (depsite being a neural network), perhaps with further hyperparameter tuning and training it could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the baseline test accuracy was 54%, our neural networks have shown an improvement with an accuracy of 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next?\n",
    "We have walked through how to implement MLPClassifiers, MLPRegression (autoencoder) \n",
    "- Its important to note the MLPClassifier and MLPRegression only capture a small amount of what is possible with deep learning.\n",
    "- Deep learning involves many of the concepts we have covered in BIDS, from regularisation to kernels, however it also typically requires larger datasets to extract the relevant data structures.\n",
    "- If you want to explore deep learning after completing BIDS, you can start with different frameworks such as [TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [keras](https://keras.io).\n",
    "\n",
    "For now, try to further your understanding and practice with simpler NNs:\n",
    "- Try using a different scaling for instance: ```robust_scale``` / ```RobustScaler```, ```power_transform``` / ```PowerTransformer``` as these may considerably change your model performance.\n",
    "- Change certain parameters such as latent space dimensions (number of components) in PCA.\n",
    "- Use a different dataset for a regression problem.\n",
    "- Try run a GridSearchCV on the Autoencoder model (note this will increase training time and RandomizedSearchCV will be faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
